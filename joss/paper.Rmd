---
title: 'Rclean: A Tool for Writing Cleaner, More Transparent Code'
tags:
  - R
  - reproducibility
  - transparency
  - code cleaning
  - data provenance
authors:
  - name: Matthew K. Lau
    orcid: 0000-0003-3758-2406
    affiliation: 1
  - name: Thomas F. J.-M. Pasquier
    orcid: 0000-0001-6876-1306
    affiliation: "2, 3" 
  - name: Margo Seltzer
    orcid: 0000-0002-2165-4658
    affiliation: "4"
affiliations:
 - name: Harvard Forest, Harvard University 
   index: 1
 - name: Department of Computer Science, University of Bristol 
   index: 2
 - name: School of Engineering and Applied Science, Harvard University
   index: 3
 - name: Department of Computer Science, University of British Columbia
   index: 4
date: 6 December 2019
bibliography: paper.bib
---


# Introduction

The growth of open-source statistical software programming has been
explosive in the last decade. In particular, the statistical
programming language ``R`` has grown exponentially to become one of
the top ten programming languages in use today. Recently, concerns
have arisen over the reproducibility of scientific research
[@Peng2011 @Baker2016 @Stodden2018] and the potential issues stemming
from the complexity and fragility of analytical software
[@Pasquier2017 @Chen2018]. There is now a recognition that simply
making the code open is not enough, and that there is a need for
improvements to documentation and transparency [@Chen2018]. From this
perspective, tools that can lower the time and energy required to
re-factor and streamline analytical scripts could have a significant
impact on scientific reproducibility across all disciplines
[@Visser2015]. Supporting this objective, we have created ``Rclean``
which automatically reduces a script to the parts that are
specifically relevant to a research product (e.g.  a blog, academic
talk or research article). 


At it's root R is a statistical programming language. That is, it was
designed for use in analytical workflows. As such, the majority of the
R community is focused on producing code for idiosyncratic projects
that are results oriented. Also, R's design is intentionally at a
level that abstracts many aspects of programming that would otherwise
act as a barrier to entry for many users. This is good in that there
are many people who use R to their benefit with little to no formal
training in computer science or software engineering. However, these
same users are also frequently frustrated by code that is fragile,
buggy and complicated enough to quickly become obtuse even to
themselves in a very short amount of time. In addition, when scripts
take an extremely long time to execute, being able to reduce
unnecessary analyses can help increase computation efficiency.


More often then not, when someone is writing an R script, they are
intent on getting a set of results. This set of results is always a
subset of a much larger set of possible ways to explore a dataset, as
there are many statistical approaches and tests, let alone ways to
create visualizations and other representations of patterns in
data. This commonly leads to lengthy, complicated scripts from which
researchers manually subset results, but never refactor (i.e. reduce
to the final subset). In part, this is enabled by a lack of a proper
version control system, and in order to record their process and not
lose work, the entire process remains in a single or several
scripts. Although *Rclean* is not designed to fix the latter, it can
help with the former issue, once an appropriate versioning system is
adopted (e.g. git or subversion).


# Methods


This process relies on the
generation of data provenance [@Carata2014], which is a formal
representation of the execution of a computational process
(https://www.w3.org/TR/prov-dm/), to rigorously determine the the
unique computational pathway from inputs to results. However, as the
intended user is a researcher conducting analyses, the process is
abstracted and only the minimum information is required and presented
to the user to streamline the process of creating "cleaner" code. The
output generated by ``RClean`` is the minimum and sufficient code
needed to generate the chosen result.

The workhorse behind *Rclean* is data provenance. Here, when we refer
to provenance we are talking about a formalized representation of the
computational process that produced some data. Data is used in a broad
sense, not just data that were collected in a research project. There
are multiple approaches to collecting data provenance, but *Rclean*
uses "prospective" provenance, which analyzes code and uses language
specific information to predict the relationship among processes and
data objects. *Rclean* relies on a library called *CodeDepends* to
gather the prospective provenance for each script. For more
information on the mechanics of the *CodeDepends* package, see
[@Lang2019]. To get an idea of what data provenance is, take a look at
the `code_graph` function. The plot that it generates is a graphical
representation of the prospective provenance generated for *Rclean*.

```{R prov-graph}

code_graph(script)

```

## The API

The package's main function is `clean`. 

code_graph
get_libs
get_vars
keep

# Results

## Example

Conducting analyses is challenging in that it requires thinking about
multiple concepts at the same time. What did I measure? What analyses
are relevant to them? Do I need to transform the data? How do I go
about managing the data given how they were entered? What's the code
for the analysis I want to run? And so on. Data analysis can be messy
and complicated, so it's no wonder that code reflects this. And this
is a reason why having a way to isolate code based on variables can be
valuable. The following is an example of a script that has some
complications. As you can see, although the script is not extremely
long, it's long enough to make it frustrating to visualize it in its
entirety and pick through it.


```{R long-setup, echo = FALSE, eval = TRUE}

readLines(script.long)

```

So, let's say we've come to our script wanting to extract the code to
produce one of the results `fit.sqrt.A`, which is an analysis that is
relevant to some product. Not only do we want to double check the
results, we also want to use the code again for another purpose, such
as creating a plot of the patterns supported by the test. 

Manually tracing through our code for all the variables used in the
test and finding all of the lines that were used to prepare them for
the analysis would be annoying and difficult, especially given the
fact that we have used "x" as a prefix for multiple unrelated objects
in the script. Instead, we can easily do this automatically with
*Rclean*.

```{R long}

clean(script.long, "fit.sqrt.A")

```

As you can see, *Rclean* has picked through the tangled bits of code
and found the minimal set of lines relevant to our object of
interest. This code can now be visually inspected to adapt the
original code or ported to a new, "refactored" script. 



## Software Availability

The software is currently hosted on Github, and we recommend using the
`devtools` library to install directly from the repository
(https://github.com/MKLau/rclean). The package is open-source and
welcomes contributions. Please visit the repository page to report
issues, request features or provide other feedback.

# Discussion

The ``Rclean`` package provides a simple, easy to use tool for
scientists conducting analyses in the R programming language. Using
graph analytic algorithms, ``Rclean`` isolates the code necessary to
produce a specified result (e.g., an object stored in memory or a
table or figure written to disk). As statistical programming becomes
more common across the sciences, tools that make the production of
accessible code will be an important aid for improving scientific
reproducibility. ``Rclean`` has been designed to take advantage of
recent advances in data provenance capture techniques to create a
minimal tool for this purpose.

It is worth mentioning and discussing that `Rclean` does not keep
comments present in code. Although, there is often very useful or even
invaluable information in comments, the `clean` function removes
comments. This is primarily due to the lack of a mathematically formal
method for determining their relationship to the code itself. Comments
at the end of lines are typically relevant to the line they are on,
but this is not explicitly required. Also, comments occupying their
own lines usually refer to the following lines, but this is also not
necessarily the case. As `clean` depends on the unambiguous
determination of relationships in the production of results, it cannot
operate automatically on comments. However, comments in the original
code remain untouched and can be used to inform the reduced
code. Also, as the `clean` function is oriented toward isolating code
based on a specific result, the resulting code tends to naturally
support the generation of new comments that are higher level
(e.g. "The following produces a plot of the mean response of each
treatment group."), and lower level comments are not necessary because
the code is simpler and clearer.



Although, a lot of great work can be done with type of data
provenance, there are limitations. Only using prospective provenance
means that the outcomes of some processes can not be predicted. For
example, if there is a part of a script that is determined by a random
number, the current implementation of prospective provenance can not
predict the path that will be taken through the code. Therefore, the
code cannot be reduced to exclude the pathway that would not be
taken. Such limitations can be overcome with other data provenance
methods. One solution is "retrospective" provenance, which tracks a
computational process as it is executing. Through this active
monitoring process, retrospective provenance can gather specific
information, such as the results relevant to our random number
example. Using retrospective provenance comes at a cost, however, in
that in order to gather it, the script needs to be executed. When
scripts are computationally intensive or contain bugs that stop
execution, then retrospective provenance can not be obtained for part
or all of the code. Some work has already been done in the direction
of implementing retrospective provenance for code cleaning in R (see
http://end-to-end-provenance.github.io).


For example, the existing framework could be extended to support new
provenance capture methods, and there is tremendous potential for the
use of code cleaning in the creation of more robust capsules
[@Pasquier2018].

# Acknowledgments

This work was improved by discussions with ecologists at Harvard
Forest. Much of the work was funded by US National Science Foundation
grant SSI-1450277 for applications of End-to-End Data Provenance.

# References

