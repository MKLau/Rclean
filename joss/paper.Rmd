---
title: 'Rclean: A Tool for Writing Cleaner, More Transparent Code'
tags:
  - R
  - reproducibility
  - transparency
  - code cleaning
  - data provenance
authors:
  - name: Matthew K. Lau
    orcid: 0000-0003-3758-2406
    affiliation: 1
  - name: Thomas F. J.-M. Pasquier
    orcid: 0000-0001-6876-1306
    affiliation: "2, 3" 
  - name: Margo Seltzer
    orcid: 0000-0002-2165-4658
    affiliation: "4"
affiliations:
 - name: Harvard Forest, Harvard University 
   index: 1
 - name: Department of Computer Science, University of Bristol 
   index: 2
 - name: School of Engineering and Applied Science, Harvard University
   index: 3
 - name: Department of Computer Science, University of British Columbia
   index: 4
date: 6 December 2019
bibliography: paper.bib
---


# Introduction

The growth of programming in the sciences has been explosive in the
last decade. In particular, the statistical programming language ``R``
has grown exponentially to become one of the top ten programming
languages in use today. Recently, concerns have arisen over the
reproducibility of scientific research [@Peng2011 @Baker2016
@Stodden2018] and the potential issues stemming from the complexity
and fragility of analytical software [@Pasquier2017 @Chen2018]. There
is now a recognition that simply making the code open is not enough,
and that there is a need for improvements to documentation and
transparency [@Chen2018].

At it's root R is a statistical programming language, that is, it was
designed for use in analytical workflows. As such, the majority of the
R community is focused on producing code for idiosyncratic projects
that are results oriented. Also, R's design is intentionally at a
level that abstracts many aspects of programming that would otherwise
act as a barrier to entry for many users. This is good in that there
are many people who use R to their benefit with little to no formal
training in computer science or software engineering. However, these
same users are also frequently frustrated by code that is fragile,
buggy and complicated enough to quickly become obtuse (even to
themselves) in a very short amount of time, which leads to frequently
re-writing code unecessarily. Also, when scripts take an extremely
long time to execute, reducing unnecessary analyses can increase
computational efficiency.


From this perspective, tools that can lower the time and energy
required to re-factor and streamline analytical scripts, and otherwise
help to "clean" code will have a significant impact on scientific
reproducibility across all disciplines [@Visser2015]. To support this
objective, we have created ``Rclean`` which automatically reduces a
script to the parts that are specifically relevant to a research
product (e.g.  a blog, academic talk or research article). Here, we
detail the structure of the package's API, describe the general
workflow illustrated by an example use case and provide some
background on how the underlying use of data provenance enables the
package.


# Methods


The main goal of *Rclean* is to provide a means to simplify code. To
keep the process simple and straight-forward, the API has been kept to
a minimum set of functions, which enable a user to conduct the basic
workflow of getting information about the posible results in a script,
obtaining the minimum code for a set of results and then creating a
new "cleaned" script or other software (e.g. a function, reproducible
example, web-app, etc.). 

The package's main functions are `clean` and `keep`. When provided a
file path to a script and the name of a result (or a set of results),
`clean` analyzes the script's code and extracts the lines of code
required to produce the results. This code can then be passed to the
`keep` function, which can either write the code to disk or copy the
code to the user's clipboard (if no ouptput file path is supplied) and
the user can paste the code into another location (e.g. a script
editor).

In the process of cleaning a script, it is likely that a user might
require some help analyzing the script. There are several functions to
help. The `get_vars` function will return a list of possible results
for a given script at a supplied file path. This is obviously an
important step, and justsifiably, the default behavior of the `clean`
function is to run `get_vars` if no results are supplied. To help with
limiting and checking the selection of results, the `code_graph` function
creates a network graph of the relationships among the various
results and lines of code in the script. Last, the `get_libs` function
can be used to detect the packages that a given script depends on,
which it will return as coded library calls that can be inserted into
a cleaned script.


# Results

## Example

Conducting analyses is challenging in that it requires thinking about
multiple concepts at the same time. What did I measure? What analyses
are relevant to them? Do I need to transform the data? How do I go
about managing the data given how they were entered? What's the code
for the analysis I want to run? And so on. Data analysis can be messy
and complicated, so it's no wonder that code reflects this. And this
is a reason why having a way to isolate code based on variables can be
valuable. The following is an example of a script that has some
complications. As you can see, although the script is not extremely
long, it's long enough to make it frustrating to visualize it in its
entirety and pick through it.


```{R long-setup, echo = FALSE, eval = TRUE}

readLines(script.long)

```

So, let's say we've come to our script wanting to extract the code to
produce one of the results `fit.sqrt.A`, which is an analysis that is
relevant to some product. Not only do we want to double check the
results, we also want to use the code again for another purpose, such
as creating a plot of the patterns supported by the test. 

Manually tracing through our code for all the variables used in the
test and finding all of the lines that were used to prepare them for
the analysis would be annoying and difficult, especially given the
fact that we have used "x" as a prefix for multiple unrelated objects
in the script. Instead, we can easily do this automatically with
*Rclean*.

```{R long}

clean(script.long, "fit.sqrt.A")

```

As you can see, *Rclean* has picked through the tangled bits of code
and found the minimal set of lines relevant to our object of
interest. This code can now be visually inspected to adapt the
original code or ported to a new, "refactored" script. 



## Software Availability

The software is currently hosted on Github, and we recommend using the
`devtools` library to install directly from the repository
(https://github.com/MKLau/rclean). The package is open-source and
welcomes contributions. Please visit the repository page to report
issues, request features or provide other feedback.

# Discussion

The ``Rclean`` package provides a simple, easy to use tool for
scientists conducting analyses in the R programming language. Using
graph analytic algorithms, ``Rclean`` isolates the code necessary to
produce a specified result (e.g., an object stored in memory or a
table or figure written to disk). As statistical programming becomes
more common across the sciences, tools that make the production of
accessible code will be an important aid for improving scientific
reproducibility. ``Rclean`` has been designed to take advantage of
recent advances in data provenance capture techniques to create a
minimal tool for this purpose.

It is worth mentioning and discussing that `Rclean` does not keep
comments present in code. Although, there is often very useful or even
invaluable information in comments, the `clean` function removes
comments. This is primarily due to the lack of a mathematically formal
method for determining their relationship to the code itself. Comments
at the end of lines are typically relevant to the line they are on,
but this is not explicitly required. Also, comments occupying their
own lines usually refer to the following lines, but this is also not
necessarily the case. As `clean` depends on the unambiguous
determination of relationships in the production of results, it cannot
operate automatically on comments. However, comments in the original
code remain untouched and can be used to inform the reduced
code. Also, as the `clean` function is oriented toward isolating code
based on a specific result, the resulting code tends to naturally
support the generation of new comments that are higher level
(e.g. "The following produces a plot of the mean response of each
treatment group."), and lower level comments are not necessary because
the code is simpler and clearer.



Although, a lot of great work can be done with type of data
provenance, there are limitations. Only using prospective provenance
means that the outcomes of some processes can not be predicted. For
example, if there is a part of a script that is determined by a random
number, the current implementation of prospective provenance can not
predict the path that will be taken through the code. Therefore, the
code cannot be reduced to exclude the pathway that would not be
taken. Such limitations can be overcome with other data provenance
methods. One solution is "retrospective" provenance, which tracks a
computational process as it is executing. Through this active
monitoring process, retrospective provenance can gather specific
information, such as the results relevant to our random number
example. Using retrospective provenance comes at a cost, however, in
that in order to gather it, the script needs to be executed. When
scripts are computationally intensive or contain bugs that stop
execution, then retrospective provenance can not be obtained for part
or all of the code. Some work has already been done in the direction
of implementing retrospective provenance for code cleaning in R (see
http://end-to-end-provenance.github.io).


For example, the existing framework could be extended to support new
provenance capture methods, and there is tremendous potential for the
use of code cleaning in the creation of more robust capsules
[@Pasquier2018].

# Acknowledgments

This work was improved by discussions with ecologists at Harvard
Forest. Much of the work was funded by US National Science Foundation
grant SSI-1450277 for applications of End-to-End Data Provenance.

# References

