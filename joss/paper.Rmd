---
title: 'Rclean: A Tool for Writing Cleaner, More Transparent Code'
tags:
  - R
  - reproducibility
  - transparency
  - code cleaning
  - data provenance
authors:
  - name: Matthew K. Lau
    orcid: 0000-0003-3758-2406
    affiliation: 1
  - name: Thomas F. J.-M. Pasquier
    orcid: 0000-0001-6876-1306
    affiliation: "2, 3" 
  - name: Margo Seltzer
    orcid: 0000-0002-2165-4658
    affiliation: "4"
affiliations:
 - name: Harvard Forest, Harvard University 
   index: 1
 - name: Department of Computer Science, University of Bristol 
   index: 2
 - name: School of Engineering and Applied Science, Harvard University
   index: 3
 - name: Department of Computer Science, University of British Columbia
   index: 4
date: 
bibliography: paper.bib
---

# Introduction

The growth of programming in the sciences has been explosive in the
last decade. This has facilitated the rapid advancement of scienqce
through the agile development of computational tools. However,
concerns have begun to surface about the reproducibility of scientific
research in general [@Peng2011 @Baker2016] and the potential issues
stemming from issues with analytical software [@Pasquier2017
@Stodden2018]. Specifically, there is a growing recognition across
disciplines that simply making data and software "available" is not
enough and that there is a need to improve the transparency and
stability of scientific software [@Pasquier2018].

At the core of the growth of scentific conputation, the ``R``
statistical programming language has grown exponentially to become one
of the top ten programming languages in use today. At it's root R is a
*statistical* programming language. That is, it was designed for use
in analytical workflows; and the majority of the R community is
focused on producing code for idiosyncratic projects that are
*results* oriented. Also, R's design is intentionally at a level that
abstracts many aspects of programming that would otherwise act as a
barrier to entry for many users. This is good in that there are many
people who use R to their benefit with little to no formal training in
computer science or software engineering, but these same users can
also be frequently frustrated by code that is fragile, buggy and
complicated enough to quickly become obtuse even to the authors. The
stability, reproducibility and re-use of scientific analyses in R
would be improved by refactoring. From this perspective, tools that
can lower the time and energy required to re-factor and streamline
analytical scripts and otherwise help to "clean" code, but abstracted
enough to be easily accessible, could have a significant impact on
scientific reproducibility across all disciplines [@Visser2015].

To provide support for easier refactoring in R, we have created
``Rclean``. The ``Rclean`` package provides tools to automatically
reduce a script to the parts that are specifically relevant to a
research product (e.g.  a scientific report, academic talk, research
article, etc.). Although potentially useful to all R coders, it was
designed to ease refactoring for scientists that use R but do not have
formal training in software engineering. Here, we detail the structure
of the package's API, describe the general workflow illustrated by an
example use case and provide some background on how data provenance
enables the underlying functionality of the package. We then end with
a discussion of future applications of data provenance in the context
of "code cleaning" and the potential integration with other software
engineering tools for the R community.


# Methods

More often then not, when someone is writing an R script, the intent
to produce a set of results, such as a statistical analysis, figure,
table, etc. This set of results is always a subset of a much larger
set of possible ways to explore a dataset, as there are many
statistical approaches and tests, let alone ways to create
visualizations and other representations of patterns in data. This
commonly leads to lengthy, complicated scripts from which researchers
manually subset results, but never refactor, i.e. refine code so that
it is shorter and focused on a desired product.

The goal of `Rclean` is to provide a set of tools that help someone
reduce and organize code based on results. The package uses an
automated technique based on data provenance (details below) to
analyze existing scripts and provide ways to identify and extract code
to produce a desired output. To keep the process simple and
straight-forward much of this process has been abstracted for the
user, and the API has been kept to a minimum set of functions that
enable a user to conduct the following basic workflow:

1. Obtain the "cleaned" code for a result(s).
2. Transfer the code to a new context (e.g. a new script,
function, reproducible example, web-app, etc.).
3. Get information about the posible results and repeat as needed.

## The API

The package's main functions are `clean` and `keep`. When provided a
file path to a script and the name of a result (or a set of results),
`clean` analyzes the script's code and extracts the lines of code
required to produce the results. This code can then be passed to the
`keep` function, which can either write the code to disk or copy the
code to the user's clipboard (if no ouptput file path is supplied) and
the user can paste the code into another location (e.g. a script
editor).

In the process of cleaning a script, it is likely that a user might
require some help analyzing the script. There are several functions to
help with this process. The `get_vars` function will return a list of
possible results for a given script at a supplied file path. This is
obviously an important step, and justsifiably, the default behavior of
the `clean` function is to run `get_vars` if no results are
supplied. To help with limiting and checking the selection of results,
the `code_graph` function creates a network graph of the relationships
among the various results and lines of code in the script. Last, the
`get_libs` function can be used to detect the packages that a given
script depends on, which it will return as coded library calls that
can be inserted into a cleaned script.

## Data Provenance

All of these processes rely on the generation of data provenance.  The
term provenance means information about the origins of some
object. Data provenanve is a formal representation of the execution of
a computational process (https://www.w3.org/TR/prov-dm/), to
rigorously determine the the unique computational pathway from inputs
to results [@Carata2014]. To avoid confusion, note that "data" in this
context is used in a broad sense to include all of the information
generated during computation, not just the data that are collected in
a research project that are used as input to an analysis. Having the
formalized, mathematically rigorous representation that data
provenance provides gaurantees that the analyses that *RClean*
conducts are theoretically sound. Most importantly, because the
relationships defined by the provenance can be represented as a graph,
it is possible to apply network search algorithms to determine the
minimum and sufficient code needed to generate the chosen result in
the `clean` function.

There are multiple approaches to collecting data provenance, but
`Rclean` uses "prospective" provenance, which analyzes code and uses
language specific information to predict the relationship among
processes and data objects. `Rclean` relies on a library called
`CodeDepends` to gather the prospective provenance for each
script. For more information on the mechanics of the `CodeDepends`
package, see [@Lang2019]. To get an idea of what data provenance is,
take a look at the `code_graph` function. The plot that it generates
is a graphical representation of the prospective provenance generated
for `Rclean`.

```{R load-script, echo = FALSE, results = "hide"}
library(CodeDepends)
script <- system.file(
    "example", 
    "simple_script.R", 
    package = "Rclean")
script.long <- system.file(
    "example", 
    "long_script.R", 
    package = "Rclean")

```

	```{R prov-graph, fig.cap = "Network diagram of the prospective data provenance generated for an example script. Arrows indicate relationships among objects and lines of code (numbered)."}

code_graph(script)

```

# Results

## Example

Conducting analyses is challenging in that it requires thinking about
multiple concepts at the same time. What did I measure? What analyses
are relevant to them? Do I need to transform the data? How do I go
about managing the data given how they were entered? What's the code
for the analysis I want to run? And so on. Data analysis can be messy
and complicated, so it's no wonder that code reflects this. And this
is a reason why having a way to isolate code based on variables can be
valuable. The following is an example of a script that has some
complications. As you can see, although the script is not extremely
long, it's long enough to make it frustrating to visualize it in its
entirety and pick through it.


```{R long-setup, echo = FALSE, eval = TRUE}

readLines(script.long)

```

So, let's say we've come to our script wanting to extract the code to
produce one of the results `fit.sqrt.A`, which is an analysis that is
relevant to some product. Not only do we want to double check the
results, we also want to use the code again for another purpose, such
as creating a plot of the patterns supported by the test. 

Manually tracing through our code for all the variables used in the
test and finding all of the lines that were used to prepare them for
the analysis would be annoying and difficult, especially given the
fact that we have used "x" as a prefix for multiple unrelated objects
in the script. Instead, we can easily do this automatically with
`Rclean`.

```{R long}

clean(script.long, "fit_sqrt_A")

```

As you can see, `Rclean` has picked through the tangled bits of code
and found the minimal set of lines relevant to our object of
interest. This code can now be visually inspected to adapt the
original code or ported to a new, "refactored" script. 



## Software Availability

The software is currently hosted on Github, and we recommend using the
`devtools` library to install directly from the repository
(https://github.com/ROpenSci/Rclean). The package is open-source and
welcomes contributions. Please visit the repository page to report
issues, request features or provide other feedback.

# Discussion

The ``Rclean`` package provides a simple, easy to use tool for
scientists conducting analyses in the R programming language. Using
graph analytic algorithms, ``Rclean`` isolates the code necessary to
produce a specified result (e.g., an object stored in memory or a
table or figure written to disk). As statistical programming becomes
more common across the sciences, tools that make the production of
accessible code will be an important aid for improving scientific
reproducibility. ``Rclean`` has been designed to take advantage of
recent advances in data provenance capture techniques to create a
minimal tool for this purpose.

It is worth mentioning and discussing that `Rclean` does not keep
comments present in code. Although, there is often very useful or even
invaluable information in comments, the `clean` function removes
comments. This is primarily due to the lack of a mathematically formal
method for determining their relationship to the code itself. Comments
at the end of lines are typically relevant to the line they are on,
but this is not explicitly required. Also, comments occupying their
own lines usually refer to the following lines, but this is also not
necessarily the case. As `clean` depends on the unambiguous
determination of relationships in the production of results, it cannot
operate automatically on comments. However, comments in the original
code remain untouched and can be used to inform the reduced
code. Also, as the `clean` function is oriented toward isolating code
based on a specific result, the resulting code tends to naturally
support the generation of new comments that are higher level
(e.g. "The following produces a plot of the mean response of each
treatment group."), and lower level comments are not necessary because
the code is simpler and clearer.

The existing framework could be extended to support new provenance
capture methods. One possibility is *retrospective provenance*, which
tracks a computational process as it is executing. Through this
active, concurrent monitoring, retrospective provenance can gather
information that static prospective provenance can't. Only using
prospective provenance means that the outcomes of some processes can
not be predicted. For example, if there is a part of a script that is
determined by a random number, the current implementation of
prospective provenance can not predict the path that will be taken
through the code. Therefore, the code cannot be reduced to exclude the
pathway that would not be taken. However, using retrospective provenance comes
at a cost. In order to gather it, the script needs to
be executed. When scripts are computationally intensive or contain
bugs that stop execution, then retrospective provenance can not be
obtained for part or all of the code. Some work has already been done
in the direction of implementing retrospective provenance for code
cleaning in R (see http://end-to-end-provenance.github.io).

To conclude, we hope that `Rclean` makes writing scientifi software
easier. We look forward to feedback and help with extending its
applications, particularly in the area of reproducibility, such as
using code cleaning in the creation of more robust capsules
[@Pasquier2018]. To get involved, report bugs, suggest features,
please visit the project page.

# Acknowledgments

This work was improved by discussions with ecologists at Harvard
Forest and through the helpful review provided by the ROpenSci
community, particuarly Anna Krystalli, Will Landau and Clemens
Schmid. Much of the work was funded by US National Science Foundation
grant SSI-1450277 for applications of End-to-End Data Provenance.

# References

